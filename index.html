<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content=".">
  <meta name="keywords" content="Bourbaki-7B, Theorem Proving, Goal-conditioned MDP">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Bourbaki: Self-generated and Goal-conditioned MDPS for Theorem Proving</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/favicon.svg"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>



<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Bourbaki: Self-generated and Goal-conditioned MDPS for Theorem Proving</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Matthieu Zimmer<sup>1</sup><sup>*</sup>,
            </span>
            <span class="author-block">
              Xiaotong Ji<sup>1</sup><sup>4</sup><sup>*</sup>,
            </span>
            <span class="author-block">
              Rasul Tutunov<sup>1</sup><sup>†</sup>,
            </span>
            <span class="author-block">
              Anthony Bordg<sup>2</sup><sup>†</sup>,
            </span>
            <span class="author-block">
              Jun Wang<sup>3</sup>,
            </span>
            <span class="author-block">
              Haitham Bou Ammar<sup>1</sup><sup>3</sup><sup>‡</sup>
            </span>
          </div>
          <div class="publication-meta">
            <div class="publication-affiliations is-size-6">
              <span><sup>1</sup>Huawei Noah's Ark Lab;</span>
              <span><sup>2</sup>Huawei Lagrange Center;</span>
              <span><sup>3</sup>UCL Centre for AI;</span>
              <span><sup>4</sup>Imperial College London</span>
            </div>
            <div class="publication-notes is-size-6">
              <span><sup>*</sup>Equal contribution;</span>
              <span><sup>†</sup>Equal second author;</span>
              <span><sup>‡</sup>Correspondence: haitham.ammar@huawei.com</span>
            </div>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://github.com/Bourbaki-Prover/bourbaki-prover.github.io/blob/main/static/bourbaki.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2507.02726"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <figure class="image">
        <img src="./static/images/tree_gen.png" alt="Overview of Bourbaki: self-generated and goal-conditioned MDPs">
      </figure>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Introduction</h2>
        <div class="content has-text-justified">
          <p>
            Automated theorem proving (ATP) presents a challenging test of reasoning for large language models (LLMs), particularly due to sparse rewards and long proof horizons. 
            These challenges are amplified in benchmarks like PutnamBench, which contains university-level problems requiring deep, multi-step reasoning. 
            To address this, we introduce self-generated goal-conditioned MDPs (sG-MDPs), a new framework in which agents generate and pursue their subgoals based on the evolving proof state. Given this more structured generation of goals, the resulting problem becomes more amenable to search. We then apply MCTS-like algorithms to solve the sG-MDP, instantiating our approach in Bourbaki (7B), a modular system that ensembles multiple 7B LLMs for subgoal generation and tactic synthesis. On PutnamBench, Bourbaki (7B) solves 26 problems, achieving new state-of-the-art results with models at this scale.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

<!-- Method Section -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
        <div class="content has-text-justified">
          <p>
            We formulate theorem proving as a self-generated goal-conditioned Markov Decision Process (sG-MDP), where the agent dynamically proposes and solves subgoals based on the evolving proof state. Each action can either introduce a new conjecture or apply a tactic to resolve an existing goal.
          </p>
          <p>
            At each proof step, Bourbaki uses Monte Carlo Tree Search (MCTS) to balance exploration of novel subgoal conjectures with exploitation of promising tactics. During the selection phase, the algorithm navigates the current proof tree guided by UCT scores, while expansion consists of querying an LLM to propose candidate subgoals or Lean4 tactics. Rollouts simulate partial proof attempts by sampling additional tactics or subgoals until either a proof leaf is closed or a predefined depth is reached. Finally, backpropagation updates the value estimates of each node, allowing subsequent searches to prioritize fruitful branches.
          </p>
          <p>
            To further improve robustness and diversity, Bourbaki ensembles multiple LLMs: DeepSeek-Prover-v2–7B and Kimina-7B. For each expansion or rollout query, we aggregate their outputs by ranking subgoal proposals according to a learned confidence score, breaking ties by selecting the shorter proof path. This ensemble approach leverages the complementary strengths of different models, reducing the risk of getting stuck on erroneous or trivial subgoals. In the experiment, we also explore augmenting MCTS with policy and value networks trained via reinforcement learning to accelerate convergence on large, competition-level benchmarks.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Experiments Section -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Experiments</h2>
        <div class="content has-text-justified">
          <p>
            We evaluate <strong>Bourbaki (7B)</strong> on the PutnamBench dataset, comparing our approach against recent 7B and 8B provers across both tree search and whole-proof generation methods, as shown in Table. Bourbaki establishes a new state-of-the-art for 7B models on PutnamBench, solving 26/658 theorems at pass@512, outperforming the previous best of 23/658 solved theorems at pass@1024 by DeepSeek-Prover-V2-7B and the previous 7B state-of-the-art model on the Putnambench leaderboard Kimina-7B with 10/644 solved theorems. Our method completes more proofs with fewer samples, and notably, it discovers some new theorems at pass@512 that are not found at pass@1024 using the base models, demonstrating the strength of our goal-conditioned formulation in enhancing reasoning capabilities.
          </p>
        </div>

        <!-- Table -->
        <div class="table-container">
          <table class="table is-bordered is-striped is-hoverable is-fullwidth">
            <thead>
              <tr>
                <th>Method</th>
                <th>Model size</th>
                <th>Sample budget</th>
                <th># Solved theorems</th>
              </tr>
            </thead>
            <tbody>
              <tr><th colspan="4"><em>Tree Search Methods</em></th></tr>
              <tr>
                <td>InternLM2.5-StepProver</td>
                <td>7B</td>
                <td>256 × 32 × 600</td>
                <td>6/644</td>
              </tr>
              <tr>
                <td>ABEL</td>
                <td>8B</td>
                <td>596</td>
                <td>7/644</td>
              </tr>

              <tr><th colspan="4"><em>Whole-proof Generation Methods</em></th></tr>
              <tr>
                <td>Goedel-Prover-SFT</td>
                <td>7B</td>
                <td>512</td>
                <td>7/644</td>
              </tr>
              <tr>
                <td>STP</td>
                <td>7B</td>
                <td>64</td>
                <td>7/644</td>
              </tr>
              <tr>
                <td></td>
                <td></td>
                <td>3200</td>
                <td>8/644</td>
              </tr>
              <tr>
                <td>Kimina-Prover-Preview-Distill</td>
                <td>7B</td>
                <td>192</td>
                <td>10/644</td>
              </tr>
              <tr>
                <td>DeepSeek-Prover-V2 (non-CoT)</td>
                <td>7B</td>
                <td>128</td>
                <td>15/658</td>
              </tr>
              <tr>
                <td></td>
                <td></td>
                <td>1024</td>
                <td>23/658</td>
              </tr>
              <tr>
                <td>DeepSeek-Prover-V2 (CoT)</td>
                <td>7B</td>
                <td>128</td>
                <td>10/658</td>
              </tr>
              <tr>
                <td></td>
                <td></td>
                <td>1024</td>
                <td>11/658</td>
              </tr>
              <tr class="has-background-light">
                <td><strong>Bourbaki (7B)</strong></td>
                <td>7B</td>
                <td>512</td>
                <td><strong>26/658</strong></td>
              </tr>
            </tbody>
          </table>
        </div>

      </div>
    </div>
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{bourbaki 2025,
      title = {Bourbaki: Self-Generated and Goal-Conditioned MDPs for Theorem Proving},
      author = {Zimmer, Matthieu and Ji, Xiaotong and Tutunov, Rasul and Bordg, Anthony and Wang, Jun and Bou Ammar, Haitham},
      note = {Equal contribution: Matthieu Zimmer and Xiaotong Ji. Equal second authors: Rasul Tutunov and Anthony Bordg. Correspondence to: Haitham Bou Ammar <haitham.ammar@huawei.com>},
      year = {2025}
    }</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="columns is-centered">
    <div class="column is-8">
      <div class="content">
        <p>
          This website is adapted from <a href="https://nerfies.github.io/">Nerfies</a>, licensed under a <a rel="license"
            href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
        </p>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
